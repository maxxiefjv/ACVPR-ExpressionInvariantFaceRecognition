{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../Data/CK_CK+/CK+/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutralised_path = \"NeutralisedImages/CK_CK+/CK+/\" + 'cohn-kanade-images'\n",
    "all_neutralisedImage_files = glob.glob(neutralised_path+'/**/*.png', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emotions = {0: 'neutral', 1: 'anger', 2: 'contempt', 3: 'disgust', \n",
    "                4: 'fear', 5: 'happy', 6: 'sadness', 7: 'surprise'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Image():\n",
    "    \n",
    "    def __init__(self, image, landmarks, emotion, name):\n",
    "        self.image = image\n",
    "        self.original_image = copy.deepcopy(self.image)\n",
    "        self.name = name\n",
    "        self.emotion = emotion\n",
    "        self.landmarks = landmarks\n",
    "        self.neutralised_image = np.array([])\n",
    "        self.hash = hash(self)\n",
    "    \n",
    "    def getEmotion(self):\n",
    "        return self.emotion\n",
    "    \n",
    "    def getLandmarks(self):\n",
    "        return self.landmarks\n",
    "    \n",
    "    def setNeutralisedImage(self, image):\n",
    "        self.neutralised_image = image\n",
    "        \n",
    "    def getNeutralisedImage(self):\n",
    "        return self.neutralised_image\n",
    "        \n",
    "    def getName(self):\n",
    "        return self.name\n",
    "        \n",
    "    def getImage(self):\n",
    "        return self.image\n",
    "    \n",
    "    def clearImage(self):\n",
    "        self.image= copy.deepcopy(self.original_image)\n",
    "        return self.image\n",
    "    \n",
    "    def getHash(self):\n",
    "\n",
    "        return self.hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_det = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "face_det2 = cv2.CascadeClassifier(cv2.data.haarcascades +\"haarcascade_frontalface_alt2.xml\")\n",
    "face_det3 = cv2.CascadeClassifier(cv2.data.haarcascades +\"haarcascade_frontalface_alt.xml\")\n",
    "face_det4 = cv2.CascadeClassifier(cv2.data.haarcascades +\"haarcascade_frontalface_alt_tree.xml\")\n",
    "\n",
    "def detect_face(f, ld):\n",
    "    ''' function to detect face, crop the image and resize it and resize the corresponding landmarks too '''\n",
    "    frame = cv2.imread(f) #Open image\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #Convert image to grayscale\n",
    "    face = face_det.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    face2 = face_det2.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    face3 = face_det3.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    face4 = face_det4.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "#     ld = np.array([np.array(i) for i in ld])\n",
    "    crop_ld = ld\n",
    "    #Go over detected faces, stop at first detected face, return empty if no face.\n",
    "    if len(face) == 1:\n",
    "        facefeatures = face\n",
    "    elif len(face2) == 1:\n",
    "        facefeatures = face2\n",
    "    elif len(face3) == 1:\n",
    "        facefeatures = face3\n",
    "    elif len(face4) == 1:\n",
    "        facefeatures = face4\n",
    "    else:\n",
    "        facefeatures = \"\"\n",
    "    for (x, y, w, h) in facefeatures:\n",
    "        fw, fh = w, h\n",
    "        if any(ld[:,0] > x+w):\n",
    "            fw = int(max(ld[:,0]-x))\n",
    "        if any(ld[:,1] > y+h):\n",
    "            fh = int(max(ld[:,1]-y))\n",
    "#         gray = gray[y:y+h, x:x+w] #Cut the frame to size\n",
    "        gray = gray[y:y+fh+5, x:x+fw+5] #Cut the frame to size to max of landmark with padding of 5px\n",
    "        ld_rescale_factor = (gray.shape[1]/crop_imsize, (gray.shape[0]/crop_imsize))\n",
    "        crop_ld = np.array(list(zip(ld[:,0]-x, ld[:, 1]-y)))\n",
    "    try:\n",
    "        output = cv2.resize(gray, (crop_imsize, crop_imsize))\n",
    "        crop_ld = crop_ld/ld_rescale_factor\n",
    "        cv2.imwrite(crop_path+'/crop_'+f.split('/')[-1], output)\n",
    "    except:\n",
    "        output = gray\n",
    "\n",
    "    return output, crop_ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairPlot(im1, im2, clear_figure=False, title=''):\n",
    "    if not clear_figure:\n",
    "        plt.figure()\n",
    "    \n",
    "    fig, axes = plt.subplots(1,2)\n",
    "    axes[0].imshow(im1)\n",
    "    axes[1].imshow(im2)\n",
    "\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "def getTrianglePts(triangle_points_array):\n",
    "    return (tuple(triangle_points_array[0:2].astype(np.int32)), tuple(triangle_points_array[2:4].astype(np.int32)), tuple(triangle_points_array[4:6].astype(np.int32)))\n",
    "\n",
    "def getTriangleImage(im, triangle_points_array):\n",
    "    pt1, pt2, pt3 = getTrianglePts(triangle_points_array)\n",
    "    triangle = np.array([pt1, pt2, pt3])\n",
    "    \n",
    "    triangle_im = im\n",
    "    triangle_mask = np.zeros_like(im)\n",
    "\n",
    "    triangle_mask_fill_points = np.array([\n",
    "            [pt1[0], pt1[1]],\n",
    "            [pt2[0], pt2[1]],\n",
    "            [pt3[0], pt3[1]]\n",
    "        ], np.int32)\n",
    "    cv2.fillConvexPoly(triangle_mask, triangle_mask_fill_points, (255,255,255))\n",
    "        \n",
    "    cropped_triangle = cv2.bitwise_and(triangle_mask, triangle_im)\n",
    "    return cropped_triangle\n",
    "\n",
    "def cropByLandmarks(im,ld):\n",
    "    landmarks = list(map(lambda x: tuple(x),ld))\n",
    "\n",
    "    im_shape = im.shape\n",
    "    \n",
    "    rect = (0,0,im_shape[1],im_shape[0])\n",
    "    subdiv = cv2.Subdiv2D(rect)\n",
    "    subdiv.insert(landmarks)\n",
    "    \n",
    "    triangles = subdiv.getTriangleList()\n",
    "    cropped_image = np.zeros_like(im) \n",
    "    for triangle in triangles:\n",
    "        triangle_image = getTriangleImage(im, triangle)      \n",
    "        cropped_image = cv2.add(cropped_image, triangle_image)\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# crop_imsize = 350\n",
    "# subjects = []\n",
    "\n",
    "# with tqdm(total=len(all_neutralisedImage_files), desc=\"Processing entries\") as pbar:\n",
    "\n",
    "#     for neutral_image_path in all_neutralisedImage_files[:]:\n",
    "\n",
    "#         original_image_path = \"../Data/\" + \"/\".join(neutral_image_path.split(\"/\")[1:])\n",
    "#         landmark_path = base_path + \"Landmarks/\" + \"/\".join(neutral_image_path.split(\"/\")[4:])[:-4] + '_landmarks.txt'\n",
    "#         emotion_path = base_path + \"Emotion/\" + \"/\".join(neutral_image_path.split(\"/\")[4:-1])\n",
    "        \n",
    "#         emotion_files = glob.glob(emotion_path+'/*.txt')\n",
    "\n",
    "#         if len(emotion_files) < 1:\n",
    "#             pbar.update(1)\n",
    "#             continue\n",
    "\n",
    "#         with open(emotion_files[0]) as emotion_file:\n",
    "#             emotion = emotion_file.read()\n",
    "#             emotion = all_emotions[(int(float(emotion)))]\n",
    "\n",
    "#         landmarks = np.loadtxt(landmark_path)\n",
    "        \n",
    "#         original_image, resized_landmarks = detect_face(original_image_path, landmarks)  \n",
    "#         original_image = cv2.resize(original_image,(crop_imsize, crop_imsize))\n",
    "#         original_image = cropByLandmarks(original_image, resized_landmarks)\n",
    "\n",
    "#         neutralised_image = cv2.imread(neutral_image_path) #Open Neutralised image\n",
    "# #         pairPlot(original_image, neutralised_image)\n",
    "#         im_object = Image(original_image, landmarks, emotion, int((original_image_path.split(\"/\")[-1])[:-4].split('_')[0][1:]))\n",
    "#         im_object.setNeutralisedImage(neutralised_image)\n",
    "\n",
    "#         subjects.append(im_object)\n",
    "#         pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half of the images do not have an emotion???!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets for face recognition and exp recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_data = []\n",
    "\n",
    "# for image in subjects:\n",
    "#     images_data.append(np.array([image.getNeutralisedImage(), image.getImage(), image.getEmotion(), image.getName()]))\n",
    "\n",
    "# images_df = pd.DataFrame(images_data, columns=['neutralised_image', 'original_image', 'original_emotion', 'name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_df.to_pickle('./joined_images_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df = pd.read_pickle('./joined_images_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df = images_df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((670, 2), (330, 2), (670,), (330,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_rec_origin, y_rec_origin = images_df[['original_image', 'neutralised_image']], images_df['name']\n",
    "\n",
    "X_rec_origin_train, X_rec_origin_test, y_rec_origin_train, y_rec_origin_test = train_test_split(X_rec_origin, y_rec_origin, test_size=0.33, random_state=42)\n",
    "X_rec_origin_train.reset_index(drop=True, inplace=True), X_rec_origin_test.reset_index(drop=True, inplace=True)\n",
    "y_rec_origin_train.reset_index(drop=True, inplace=True), y_rec_origin_test.reset_index(drop=True, inplace=True)\n",
    "X_rec_origin_train.shape, X_rec_origin_test.shape, y_rec_origin_train.shape, y_rec_origin_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 54, 117,  37,  53, 111, 117,  54,  65,  55,  65,  65,  62, 127,\n",
       "       116,  62,  62,  65,  52, 116,  99, 119, 128,  52, 129, 128, 127,\n",
       "       117,  62, 117, 127,  62,  91, 127,  62, 127, 111,  37, 119,  62,\n",
       "        55, 127,  54, 127, 127, 111,  37,  65, 129,  52,  55, 126, 127,\n",
       "        55, 126, 119,  52,  65, 127,  90,  55, 119, 129,  54, 116,  91,\n",
       "       129,  62,  62,  37,  90, 111, 895,  65,  65,  53,  54,  52, 110,\n",
       "       129,  65,  65,  52,  55, 126,  55,  54,  54,  55,  65,  62, 111,\n",
       "        37,  65,  91,  62,  96, 129, 129,  98, 128, 128,  96,  96,  62,\n",
       "       128, 111, 119,  96,  52,  53, 119,  91,  99,  98,  65,  91,  53,\n",
       "       127,  54,  54,  91,  55,  62, 127, 126,  53,  52,  54,  96,  62,\n",
       "       129, 116,  53,  99,  62, 116, 116,  62,  65,  62,  62, 116,  52,\n",
       "       116, 117,  90,  52,  52,  53,  53,  53, 116, 111,  62,  65, 129,\n",
       "       129, 117, 127,  65,  52,  53,  37, 116,  62, 119,  62,  62,  55,\n",
       "        52,  54, 119,  62,  52,  55,  99,  54,  62,  99,  37, 126,  65,\n",
       "       117, 111, 117,  37, 119, 119,  90, 111,  96,  54,  55, 110,  62,\n",
       "        65,  55, 119,  53, 116,  91, 129,  53, 128,  55,  62, 127, 129,\n",
       "        37,  62, 119, 128, 126,  65, 116,  99,  65,  55,  53, 127, 116,\n",
       "       895,  54, 129,  55,  52,  96, 126, 129, 110, 119,  90,  55,  37,\n",
       "       126,  62,  54, 117, 110,  99, 127, 116,  55, 129,  96,  52, 127,\n",
       "       895,  62,  53, 117, 111,  55,  55,  55,  62,  65,  53, 129, 127,\n",
       "       126,  55,  54,  37,  55, 127, 128,  53,  91,  37, 111, 127, 117,\n",
       "       126,  98,  99, 119, 116,  53,  53, 119, 117,  37,  37,  37,  54,\n",
       "        90,  65,  90, 129,  52, 127,  55, 127, 111,  65, 129,  99,  99,\n",
       "        65,  99,  65, 128,  37,  52, 119, 111,  99,  62,  99,  96, 119,\n",
       "       111,  99,  96, 895,  99,  55,  54,  55, 119,  55,  55,  65,  99,\n",
       "        55,  91,  99, 127,  52, 127,  55,  90,  37, 116, 119,  54, 116,\n",
       "        62,  62,  37,  91,  55,  54, 116, 129, 127,  62,  99, 119,  98,\n",
       "        55,  37, 111,  55, 111,  62,  98, 126, 117,  37,  91,  53,  37,\n",
       "        52, 126,  53, 128,  62, 127,  55,  98,  52,  65,  52,  65, 127,\n",
       "       127,  37,  52,  37,  98, 110,  62, 117, 119, 116,  55,  53, 127,\n",
       "        99, 119,  53, 128, 128,  62, 116, 119, 119,  62, 126,  62,  52,\n",
       "        37,  52,  55,  52, 127,  98,  62,  55,  37,  37,  52, 119,  98,\n",
       "        52,  37,  55,  52,  53,  55,  37,  62,  99,  65, 126,  55,  62,\n",
       "        65, 116,  54, 126,  55,  53,  54,  96,  90,  55, 126,  55,  54,\n",
       "       117,  98,  62,  52,  54,  96, 126,  37,  53,  53, 128,  96,  62,\n",
       "       119,  99,  55, 895, 110,  65,  98,  90,  52,  52,  53,  55,  55,\n",
       "        98,  55,  91,  99,  91,  55, 129, 126,  54, 127,  65, 111,  65,\n",
       "       127,  98,  52,  55,  55,  99,  37, 116,  91, 126,  37,  91,  54,\n",
       "        62,  65,  52, 119,  62,  55,  62,  37,  55, 119, 111,  37,  98,\n",
       "        52,  53,  37,  65,  37,  54, 116,  55,  37,  91,  62,  65, 129,\n",
       "       111,  55,  55, 119, 127, 111,  52, 126,  65,  37,  96, 128,  62,\n",
       "        54,  62,  53,  65, 127, 129, 895,  98, 127,  99,  65,  55,  99,\n",
       "       129,  55, 111, 127, 129,  65,  96, 119,  37, 119,  55, 129,  52,\n",
       "       117,  37,  37,  37, 128,  54, 126,  37,  62,  62,  90,  55,  55,\n",
       "       126, 119, 127,  91,  37,  62,  54,  65,  55,  37, 116,  91,  62,\n",
       "       111,  99,  90,  91, 129, 119, 111,  55,  65,  62,  62,  96, 116,\n",
       "       110,  52,  91, 110,  55, 126, 111, 117,  53,  62, 128, 127,  55,\n",
       "        90,  98,  55,  65,  90,  65,  55, 126,  91,  55,  91,  54, 119,\n",
       "        55,  54,  65,  54,  65, 129,  65,  91,  55,  37, 126, 111, 117,\n",
       "        62, 127, 128,  99, 119,  62,  55,  62,  65, 110,  37, 117,  98,\n",
       "       127,  98,  55, 116,  96, 129,  62, 117, 119,  62, 116,  53, 111,\n",
       "        54, 129, 116, 126,  55,  62, 116])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_rec_origin_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fishface = cv2.face.FisherFaceRecognizer_create() #Initialize fisher face classifier\n",
    "fishface.train(X_rec_origin_train['original_image'].to_numpy(), np.array(y_rec_origin_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "for cnt, image in enumerate(X_test['original_image']):\n",
    "    pred, conf = fishface.predict(image)\n",
    "    if pred == np.array(y_test)[cnt]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "print('Emotion Acc: ', (100*correct)/(correct + incorrect))\n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for cnt, image in enumerate(X_test['neutralised_image']):\n",
    "    pred, conf = fishface.predict(image)\n",
    "    if pred == np.array(y_rec_origin_test)[cnt]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "print('Emotionless Acc: ', (100*correct)/(correct + incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fishface.predict(X_rec_origin_test['original_image'][10]), y_rec_origin_test[10], \n",
    "      fishface.predict(X_rec_origin_test['neutralised_image'][10]), y_rec_origin_test[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenface = cv2.face.EigenFaceRecognizer_create()\n",
    "eigenface.train(X_train['im_neutral_crop'], np.array(y_train))\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for cnt, image in enumerate(X_test['im_emotion_crop']):\n",
    "    pred, conf = eigenface.predict(image)\n",
    "    if pred == np.array(y_test)[cnt]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "print('Emotion Acc: ', (100*correct)/(correct + incorrect))\n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for cnt, image in enumerate(X_test['im_emotionless']):\n",
    "    pred, conf = eigenface.predict(image)\n",
    "    if pred == np.array(y_test)[cnt]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "print('Emotionless Acc: ', (100*correct)/(correct + incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eigenface.predict(X_test['im_emotion_crop'][10]), y_test[10], \n",
    "      eigenface.predict(X_test['im_emotionless'][10]), y_test[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LBHP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbph_face = cv2.face.LBPHFaceRecognizer_create()\n",
    "lbph_face.train(X_train['im_neutral_crop'], np.array(y_train))\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for cnt, image in enumerate(X_test['im_emotion_crop']):\n",
    "    pred, conf = lbph_face.predict(image)\n",
    "    if pred == np.array(y_test)[cnt]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "print('Emotion Acc: ', (100*correct)/(correct + incorrect))\n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for cnt, image in enumerate(X_test['im_emotionless']):\n",
    "    pred, conf = lbph_face.predict(image)\n",
    "    if pred == np.array(y_test)[cnt]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "print('Emotionless Acc: ', (100*correct)/(correct + incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lbph_face.predict(X_test['im_emotion_crop'][10]), y_test[10], \n",
    "      lbph_face.predict(X_test['im_emotionless'][10]), y_test[10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
