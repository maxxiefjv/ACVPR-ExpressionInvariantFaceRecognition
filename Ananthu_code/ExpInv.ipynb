{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from IPython.display import HTML\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '..\\\\Data\\\\cohn-kanade-images'\n",
    "emotion_path = '..\\\\Data\\\\Emotion'\n",
    "facs_path = '..\\\\Data\\\\FACS'\n",
    "landmarks_path = '..\\\\Data\\\\Landmarks'\n",
    "crop_path = '..\\\\Data\\\\Crop'\n",
    "\n",
    "all_image_files = glob.glob(data_path+'/**/*.png', recursive=True)\n",
    "all_emotion_files = glob.glob(emotion_path+'/**/*.txt', recursive=True)\n",
    "all_facs_files = glob.glob(facs_path+'/**/*.txt', recursive=True)\n",
    "all_landmarks_files = glob.glob(landmarks_path+'/**/*.txt', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [], [], [])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONLY 327 of the 593 sequences have emotion sequences\n",
    "# 0-7 (i.e. 0=neutral, 1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sadness, 7=surprise)\n",
    "all_emotions = {0: 'neutral', 1: 'anger', 2: 'contempt', 3: 'disgust', \n",
    "                4: 'fear', 5: 'happy', 6: 'sadness', 7: 'surprise'}\n",
    "all_image_files[:5], all_emotion_files[:5], all_facs_files[:5], all_landmarks_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "crop_imsize = 350\n",
    "\n",
    "def get_thumbnail(path):\n",
    "    ''' function to read image'''\n",
    "    i = Image.open(path)\n",
    "#     i.thumbnail((150, 150), Image.LANCZOS)\n",
    "    return i\n",
    "\n",
    "def image_base64(im):\n",
    "    ''' function to encode image in base64 '''\n",
    "    if isinstance(im, str):\n",
    "        im = get_thumbnail(im)\n",
    "    with BytesIO() as buffer:\n",
    "        im.save(buffer, 'jpeg')\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "def image_formatter(im):\n",
    "    ''' function to return html code for printing image in dataframe '''\n",
    "    return '<img src=\"data:image/jpeg;base64,{}\">'.format(image_base64(im))\n",
    "\n",
    "def crop_image_formatter(im):\n",
    "    ''' function to read cropped image from numpy array '''\n",
    "    return '<img src=\"data:image/jpeg;base64,{}\">'.format(image_base64(Image.fromarray(np.uint8(im))))\n",
    "\n",
    "face_det = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "face_det2 = cv2.CascadeClassifier(\"haarcascade_frontalface_alt2.xml\")\n",
    "face_det3 = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\")\n",
    "face_det4 = cv2.CascadeClassifier(\"haarcascade_frontalface_alt_tree.xml\")\n",
    "\n",
    "def detect_face(f, ld):\n",
    "    ''' function to detect face, crop the image and resize it and resize the corresponding landmarks too '''\n",
    "    frame = cv2.imread(f) #Open image\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #Convert image to grayscale\n",
    "    face = face_det.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    face2 = face_det2.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    face3 = face_det3.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    face4 = face_det4.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    crop_ld = ld\n",
    "    #Go over detected faces, stop at first detected face, return empty if no face.\n",
    "    if len(face) == 1:\n",
    "        facefeatures = face\n",
    "    elif len(face2) == 1:\n",
    "        facefeatures = face2\n",
    "    elif len(face3) == 1:\n",
    "        facefeatures = face3\n",
    "    elif len(face4) == 1:\n",
    "        facefeatures = face4\n",
    "    else:\n",
    "        facefeatures = \"\"\n",
    "    for (x, y, w, h) in facefeatures:\n",
    "        gray = gray[y:y+h, x:x+w] #Cut the frame to size\n",
    "        crop_ld = np.array(list(zip(ld[:,0]-x, ld[:, 1]-y)))\n",
    "    try:\n",
    "        output = cv2.resize(gray, (crop_imsize, crop_imsize))\n",
    "        cv2.imwrite(crop_path+'/crop_'+f.split('/')[-1], output)\n",
    "    except:\n",
    "        output = gray\n",
    "\n",
    "    return output, crop_ld\n",
    "\n",
    "def print_metrics(y_test, y_pred):\n",
    "    ''' just a function from data science for printing '''\n",
    "    labels = dict(all_emotions)\n",
    "    del labels[0]\n",
    "    \n",
    "    print(labels)\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Classification Report: \\n', classification_report(y_test, y_pred, \n",
    "                                                             target_names=[l for l in labels.values()]))\n",
    "    \n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    width = np.shape(conf_mat)[1]\n",
    "    height = np.shape(conf_mat)[0]\n",
    "\n",
    "    res = plt.imshow(np.array(conf_mat), cmap=plt.cm.summer, interpolation='nearest')\n",
    "    for i, row in enumerate(conf_mat):\n",
    "        for j, c in enumerate(row):\n",
    "            if c>0:\n",
    "                plt.text(j-.2, i+.1, c, fontsize=16)\n",
    "\n",
    "    cb = fig.colorbar(res)\n",
    "    plt.title('Confusion Matrix')\n",
    "    _ = plt.xticks(range(len(labels)), [l for l in labels.values()], rotation=90)\n",
    "    _ = plt.yticks(range(len(labels)), [l for l in labels.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading all the image files, and then finding corresponding landmark, facs files and save them in a dictionary\n",
    "image_dict = {}\n",
    "\n",
    "for i_im in all_image_files:\n",
    "    im_dict = {}\n",
    "    im_folder = i_im.rsplit('\\\\', 1)[0]\n",
    "    if im_folder not in image_dict.keys():\n",
    "        im_files = glob.glob(im_folder+'/*.png')\n",
    "        # print(i_im)\n",
    "        # print(im_files)\n",
    "        # print(im_folder)\n",
    "        im_dict['emotion'] = im_files[-1]\n",
    "        im_dict['neutral'] = im_files[0]\n",
    "\n",
    "        im_folder_base = im_files[-1].split('\\\\', 3)[-1].rsplit('\\\\', 1)[0]\n",
    "        # print(im_folder_base)\n",
    "        facs_folder = facs_path + '\\\\' + im_folder_base\n",
    "        emotion_folder = emotion_path + '\\\\' + im_folder_base\n",
    "        landmarks_folder = landmarks_path + '\\\\' + im_folder_base\n",
    "        # print(facs_folder)\n",
    "        facs_file = glob.glob(facs_folder+'/*.txt')[0]\n",
    "        emotion_file = glob.glob(emotion_folder+'/*.txt')\n",
    "        neutral_landmarks_file = glob.glob(landmarks_folder+'/*.txt')[0]\n",
    "        emotion_landmarks_file = glob.glob(landmarks_folder+'/*.txt')[-1]\n",
    "\n",
    "        with open(facs_file) as f:\n",
    "            data = f.read()\n",
    "            im_dict['facs'] = np.array([list(map(float, i.split())) for i in data.strip().split('\\n')])\n",
    "\n",
    "        if len(emotion_file) > 0:\n",
    "            emotion_file = emotion_file[0]\n",
    "            with open(emotion_file) as f:\n",
    "                data = f.read()\n",
    "                im_dict['label'] = int(float(data))\n",
    "                im_dict['label_str'] = all_emotions[im_dict['label']]\n",
    "\n",
    "        with open(neutral_landmarks_file) as f:\n",
    "            data = f.read()\n",
    "            im_dict['neutral_landmarks'] = np.array([list(map(float, \n",
    "                                                     i.split())) for i in data.strip().split('\\n')])\n",
    "\n",
    "        with open(emotion_landmarks_file) as f:\n",
    "            data = f.read()\n",
    "            im_dict['emotion_landmarks'] = np.array([list(map(float, \n",
    "                                                     i.split())) for i in data.strip().split('\\n')])\n",
    "\n",
    "    #     print(im_dict)\n",
    "        image_dict[im_folder] = (im_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'emotion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7ee2875ca649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# function to store images in df column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'im_emotion'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_thumbnail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'im_neutral'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneutral\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_thumbnail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda2/envs/PGround/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'emotion'"
     ]
    }
   ],
   "source": [
    "# converting that dictionary to pandas dataframe, and applying operations on certain columns\n",
    "df_images = pd.DataFrame.from_dict(image_dict).T\n",
    "df_images.to_csv('ImageDict.csv', index=False)\n",
    "\n",
    "# function to store images in df column\n",
    "df_images['im_emotion'] = df_images.emotion.map(lambda f: get_thumbnail(f))\n",
    "df_images['im_neutral'] = df_images.neutral.map(lambda f: get_thumbnail(f))\n",
    "\n",
    "# function to detect faces and crop images and store in df, and also shift landmarks to cropped face\n",
    "# df_images['im_emotion_crop'] = df_images.emotion.map(lambda f: detect_face(f))\n",
    "# df_images['im_neutral_crop'] = df_images.neutral.map(lambda f: detect_face(f))\n",
    "emotion_crop_tuple = df_images.apply(lambda row: detect_face(row.emotion, \n",
    "                                                             row.emotion_landmarks), axis=1)\n",
    "df_images[['im_emotion_crop', 'emotion_crop_landmarks']] = pd.DataFrame(emotion_crop_tuple.tolist(), index=emotion_crop_tuple.index)\n",
    "neutral_crop_tuple = df_images.apply(lambda row: detect_face(row.neutral, \n",
    "                                                             row.neutral_landmarks), axis=1)\n",
    "df_images[['im_neutral_crop', 'neutral_crop_landmarks']] = pd.DataFrame(neutral_crop_tuple.tolist(), index=neutral_crop_tuple.index)\n",
    "\n",
    "df_images.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying PIL.Image objects embedded in dataframe\n",
    "HTML(df_images[['label', 'im_neutral', 'im_neutral_crop', 'im_emotion', \n",
    "                'im_emotion_crop']].head(5).to_html(formatters={'im_neutral': image_formatter, \n",
    "                                                                'im_emotion': image_formatter,\n",
    "                                                                'im_neutral_crop': crop_image_formatter, \n",
    "                                                                'im_emotion_crop': crop_image_formatter}, escape=False))\n",
    "# HTML(image_formatter(df_images[['im_neutral']].head(1)['im_neutral'][0]))\n",
    "# df_images[['im_neutral']].head(1)['im_neutral'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld, im = df_images.head(1)[['neutral_landmarks', 'neutral']].values[0]\n",
    "implot = plt.imshow(Image.open(im))\n",
    "\n",
    "plt.scatter([ld[i][0] for i in range(68)], [ld[i][1] for i in range(68)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld, im = df_images.head(1)[['neutral_crop_landmarks', 'im_neutral_crop']].values[0]\n",
    "implot = plt.imshow(im)\n",
    "\n",
    "plt.scatter([ld[i][0] for i in range(68)], [ld[i][1] for i in range(68)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld, im = df_images.head(1)[['emotion_landmarks', 'emotion']].values[0]\n",
    "implot = plt.imshow(Image.open(im))\n",
    "\n",
    "plt.scatter([ld[i][0] for i in range(68)], [ld[i][1] for i in range(68)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld, im = df_images.head(1)[['emotion_crop_landmarks', 'im_emotion_crop']].values[0]\n",
    "implot = plt.imshow(im)\n",
    "\n",
    "plt.scatter([ld[i][0] for i in range(68)], [ld[i][1] for i in range(68)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnn = df_images[~df_images['label'].isna()][['im_emotion_crop', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_cnn['im_emotion_crop'], df_cnn['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flip = X.apply(lambda x: np.fliplr(x))\n",
    "plt.imshow(X_flip[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total = pd.concat([X.reset_index(drop=True), X_flip.reset_index(drop=True)], axis=0).reset_index(drop=True)\n",
    "X_total = np.array([i for i in X_total])\n",
    "X_total = X_total.reshape(X_total.shape[0], crop_imsize, crop_imsize, 1)\n",
    "y_total = to_categorical(pd.concat([y, y], axis=0).reset_index(drop=True))\n",
    "\n",
    "X_total.shape, y_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the test samples to be used later\n",
    "np.save('modXtest', X_test)\n",
    "np.save('modytest', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1], X_train.shape[2], X_train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 64\n",
    "num_labels = 8\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "width = X_train.shape[1]\n",
    "height = X_train.shape[2]\n",
    "colour_channels = X_train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(num_features, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(width, height, colour_channels), kernel_regularizer=l2(0.01)))\n",
    "classifier.add(Conv2D(num_features, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2), strides=(2, 2)))\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Adding another set of layers\n",
    "classifier.add(Conv2D(2*num_features, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Conv2D(2*num_features, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2), strides=(2, 2)))\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "# Adding a third set of layers\n",
    "classifier.add(Conv2D(2*2*num_features, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Conv2D(2*2*num_features, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2), strides=(2, 2)))\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "# Adding a fourth set of layers\n",
    "classifier.add(Conv2D(2*2*2*num_features, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Conv2D(2*2*2*num_features, (3, 3), padding='same', activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2), strides=(2, 2)))\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(activation = \"relu\",units = 2*2*2*num_features))\n",
    "classifier.add(Dropout(0.4))\n",
    "classifier.add(Dense(activation = \"relu\",units = 2*2*num_features))\n",
    "classifier.add(Dropout(0.4))\n",
    "classifier.add(Dense(activation = \"relu\",units = 2*num_features))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(activation = \"softmax\",units = num_labels))\n",
    "\n",
    "classifier.summary()\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7), loss = categorical_crossentropy, metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-2da3249e8e1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'true' is not defined"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "classifier.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, verbose=1, batch_size= batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
